# -*- coding: utf-8 -*-
"""SPAM_CLASSIFICATION_NOV_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Db4TNUPWqs9i57sXJFIO2Kj1etUQnOb

## ðŸ“Œ **Problem Statement: SMS Spam Detection using Logistic Regression**

### âœ… Business Context:

In the era of digital communication, mobile users frequently receive unsolicited SMS messages commonly known as **spam**. These spam messages are not just annoying but may also pose **security threats** like phishing or fraud.

Telecommunication companies and mobile app providers want to **automatically filter spam messages** to **protect users** and **improve user experience**.

---

### âœ… Objective:

To **develop a classification model** that can **automatically classify incoming SMS messages as either**:

* **"Spam" (Unwanted Message)**
  OR
* **"Ham" (Legitimate Message)**

based on the **text content** of the message.

---

### âœ… Type of Problem:

* **Supervised Machine Learning**
* **Binary Classification**
* **Algorithm Chosen:** Logistic Regression

---

### âœ… Dataset:

* Contains two columns:

  * **Text:** The content of the SMS.
  * **Label:** `spam` or `ham`.

You can use a publicly available dataset like the **"SMS Spam Collection Dataset" (UCI Machine Learning Repository)**.

---

### âœ… Steps Involved:

| Step | Task                                                                                             |
| ---- | ------------------------------------------------------------------------------------------------ |
| 1    | Data Collection                                                                                  |
| 2    | Text Preprocessing (Tokenization, Lowercasing, Removing Stopwords, Lemmatization/Stemming, etc.) |
| 3    | Feature Extraction using **TF-IDF Vectorization**                                                |
| 4    | Splitting Data into Train and Test sets                                                          |
| 5    | Building Logistic Regression Model                                                               |
| 6    | Model Evaluation using **Accuracy**, **Precision**, **Recall**, **F1-Score**, **ROC-AUC**        |
| 7    | Testing model on new SMS inputs                                                                  |

---

### âœ… Expected Outcome:

âœ… The model should predict whether a new incoming SMS is likely to be **spam** or **ham**, with a reasonable level of **accuracy and precision**, ensuring **minimum false positives (ham classified as spam)**.

---

### âœ… Business Impact:

* **Increased user trust** in mobile communication.
* **Reduced spam load** on mobile networks.
* **Better user engagement** for SMS-based apps.

---

âœ… If you want, I can help with a **full Jupyter Notebook template for this project using TF-IDF and Logistic Regression**. Want me to? ðŸ˜Š

* [1. IMPORTING LIBRARIES](#1)
    
* [2. LOADING DATA](#2)
    
* [3. DATA EXPLORATION](#3)
    * [3.1 FEATURE ENGINEERING](#3.1)
    * [3.2 OUTLIER DETECTION](#3.2)
    
    
* [4. DATA PREPREPROCESSING](#4)
    
    * [4.1 CLEANING TEXT](#4.1)
    * [4.2 TOKENIZATION](#4.2)
    * [4.3 REMOVING STOPWORDS](#4.3)
    * [4.4 LEMMATIZATION](#4.4)
    
    
* [5. VECTORIZATION](#5)
      
* [6. MODEL BUILDING](#6)
    
* [7. EVALUATING MODELS](#7)
    
* [8. END](#8)

<p style="background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;font-size:150%;text-align:center;border-radius:20px 60px;">START</p>
"""

import pandas as pd
import numpy as np
import seaborn as sns

#NLP TOOLS
import nltk
nltk.download('punkt_tab')


import warnings
warnings.filterwarnings('ignore')

# prompt: unzip /content/archive.zip

!unzip /content/archive.zip

df = pd.read_csv('/content/spam.csv',encoding='latin1')
print("Data loaded successfully.")
df.info()

# prompt: drop 2   Unnamed: 2  50 non-null     object
#  3   Unnamed: 3  12 non-null     object
#  4   Unnamed: 4  6 non-null      object

df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)
df.info()

df.head()

# prompt: rename v1 as target and v2 as text

df = df.rename(columns={'v1': 'target', 'v2': 'text'})
pd.set_option('display.max_colwidth', None)
df.head()

"""<a id="3.1"></a>
# <p style="background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;">FEATURE ENGINEERING</p>

For the purpose of data exploration, I am creating new features

* No_of_Characters: Number of characters in the text message
* No_of_Words: Number of words in the text message
* No_of_sentence: Number of sentences in the text message  
"""

#Adding a column of numbers of charachters,words and sentences in each msg
import nltk
nltk.download('punkt_tab')
df["No_of_Characters"] = df["text"].apply(len)
df["No_of_Words"]=df.apply(lambda row: nltk.word_tokenize(row["text"]), axis=1).apply(len)
df["No_of_sentence"]=df.apply(lambda row: nltk.sent_tokenize(row["text"]), axis=1).apply(len)

df.describe().T

#PS. At this step, I tokenised the words and sentences and used the length of the same.
#More on Tokenizing later in the notebook.

tail_df = df.tail(int(len(df) * 0.25))
tail_df

# prompt: pair plot

sns.pairplot(df, hue='target')

# prompt: detect outlier range for characters

import matplotlib.pyplot as plt
# <a id="3.2"></a>
# # <p style="background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;">OUTLIER DETECTION</p>
# For outlier detection I am using the `describe().T` output. Based on the maximum values, there are likely outliers in the number of characters, words, and sentences.

# Outlier detection using the Interquartile Range (IQR) method
Q1 = df[['No_of_Characters', 'No_of_Words', 'No_of_sentence']].quantile(0.25)
Q3 = df[['No_of_Characters', 'No_of_Words', 'No_of_sentence']].quantile(0.75)
IQR = Q3 - Q1

outlier_range_lower = Q1 - 1.5 * IQR
outlier_range_upper = Q3 + 1.5 * IQR

print("Outlier Range (Lower Bound):")
print(outlier_range_lower)
print("\nOutlier Range (Upper Bound):")
print(outlier_range_upper)

# prompt: #Dropping the outliers.
# df= df[(df["No_of_Characters"]<248)]
# df.shape

#Dropping the outliers.
df= df[(df["No_of_Characters"]<248)]
df.shape

sns.pairplot(df, hue='target')

#Adding a column of numbers of charachters,words and sentences in each msg
import nltk
nltk.download('punkt_tab')
df["No_of_Characters"] = df["text"].apply(len)
df["No_of_Words"]=df.apply(lambda row: nltk.word_tokenize(row["text"]), axis=1).apply(len)
df["No_of_sentence"]=df.apply(lambda row: nltk.sent_tokenize(row["text"]), axis=1).apply(len)

df.describe().T

df.head()

"""#Data Preprocessing


*  Removing Special Characters and Numbers, Including only Alphabets


* Tokenization

* Removing Stopwords

* Lemmatization
"""

# prompt: remove special characters and numbers from text in df

import re
def remove_special_characters_and_numbers(text):
    # Remove special characters and numbers, keep only alphabets and spaces
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra spaces
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

df['clean_text'] = df['text'].apply(remove_special_characters_and_numbers)

df.head()

# prompt: tokenization

nltk.download('punkt')
df['clean_text'] = df['clean_text'].apply(lambda x: nltk.word_tokenize(x))


# prompt: count number of tokens for each message in clean text and create a new column

df['num_tokens'] = df['clean_text'].apply(len)

df.head()

print("Original text examples:")
for i in range(5):
    print(f"Row {i}: {df['text'][i]}")

print("\nCleaned and lemmatized text examples:")
for i in range(5):
    print(f"Row {i}: {df['clean_text'][i]}")

"""<a id="4.3"></a>
# <p style="background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;">REMOVING STOPWORDS</p>

**Stopwords** are frequently occurring words(*such as few, is, an, etc*). These words hold meaning in sentence structure, but do not contribute much to language processing in NLP. For the purpose of removing redundancy in our processing, I am removing those. NLTK library has a set of default stopwords that we will be removing.
"""

# prompt: remove stopwords

from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
  return [word for word in tokens if word.lower() not in stop_words]

df['clean_text'] = df['clean_text'].apply(remove_stopwords)


df['num_tokens'] = df['clean_text'].apply(len)

df.head()

print("Original text examples:")
for i in range(5):
    print(f"Row {i}: {df['text'][i]}")

print("\nCleaned and lemmatized text examples:")
for i in range(5):
    print(f"Row {i}: {df['clean_text'][i]}")

# prompt: lemmatization only words , it should not split

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def lemmatize_words(text):
  return [lemmatizer.lemmatize(word,pos='v') for word in text]

df['clean_text'] = df['clean_text'].apply(lemmatize_words)


df['num_tokens'] = df['clean_text'].apply(len)


df.head()

# prompt: show some words from clean text and original text to see lemmatization

print("Original text examples:")
for i in range(5):
    print(f"Row {i}: {df['text'][i]}")

print("\nCleaned and lemmatized text examples:")
for i in range(5):
    print(f"Row {i}: {df['clean_text'][i]}")

# prompt: label encoder target

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['target'] = le.fit_transform(df['target'])
df.head()

display("\nEncoded target classes:", le.classes_)

df.head()

# prompt: tfidfvectorizer

from sklearn.feature_extraction.text import TfidfVectorizer

# Convert list of tokens back to string for TF-IDF Vectorization
df['clean_text_str'] = df['clean_text'].apply(lambda x: ' '.join(x))

tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the cleaned text data
tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text_str'])

print("TF-IDF matrix shape:", tfidf_matrix.shape)

# You can now use tfidf_matrix for training your model
# The columns of the matrix correspond to the terms in the vocabulary
# tfidf_vectorizer.get_feature_names_out() will give you the list of terms

from google.colab import drive
drive.mount('/content/drive')

df.head()

# prompt: few data from tfidf_matrix

# To view a few entries from the sparse TF-IDF matrix,
# we can convert a small portion to a dense array and print it.
# We'll print the TF-IDF values for the first 5 documents and the first 10 terms.
print("\nFew entries from the TF-IDF matrix (first 5 documents, first 10 terms):")
print(tfidf_matrix[:5, :10].todense())

# To see the corresponding terms for the first 10 columns:
print("\nCorresponding terms for the first 10 columns:")
print(tfidf_vectorizer.get_feature_names_out()[:10])

# prompt: pipeline structure for nlp preprocessing steps, if we give new input  sms, it has to pass through all the given process to be applied for model training and prediction

def preprocess_sms(sms_text):
    """
    Applies the defined preprocessing steps to a single SMS message.

    Args:
        sms_text (str): The input SMS message string.

    Returns:
        str: The processed text string, ready for vectorization.
    """
    # 1. Remove special characters and numbers
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', sms_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # 2. Tokenization
    tokens = nltk.word_tokenize(cleaned_text)

    # 3. Removing Stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # 4. Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]

    # Convert tokens back to a string for vectorization
    processed_text = ' '.join(tokens)

    return processed_text

# Example usage with a new SMS:
new_sms = "Congratulations! You've won a free iPhone! Call now to claim."
processed_new_sms = preprocess_sms(new_sms)
print(f"Original SMS: {new_sms}")
print(f"Processed SMS: {processed_new_sms}")

# To apply this to a new SMS for model prediction:
# First, preprocess the new SMS
processed_sms_for_prediction = preprocess_sms(new_sms)

# Then, vectorize the processed SMS using the fitted TF-IDF vectorizer
# Note: Use transform, not fit_transform, as the vocabulary is already learned
vectorized_sms = tfidf_vectorizer.transform([processed_sms_for_prediction])

# The 'vectorized_sms' is now ready to be fed into your trained Logistic Regression model
# Assuming you have a trained model variable named 'model':
# prediction = model.predict(vectorized_sms)
# prediction_proba = model.predict_proba(vectorized_sms)

# print(f"Vectorized SMS shape: {vectorized_sms.shape}")
# print(f"Prediction: {prediction}")
# print(f"Prediction Probability: {prediction_proba}")

# prompt: make tfidf_matrix as X and concat with y to make new df

import pandas as pd
import numpy as np
# Convert the sparse TF-IDF matrix to a dense array for concatenation
tfidf_dense = tfidf_matrix.todense()

# Convert the target variable (y) to a numpy array and reshape it to be a column vector
y = df['target'].values.reshape(-1, 1)

# Concatenate the TF-IDF matrix with the target variable
# Using np.concatenate with axis=1 concatenates column-wise
# tfidf_dense needs to be a numpy array or similar dense structure for this
# y needs to be a 2D array
new_df_array = np.concatenate((tfidf_dense, y), axis=1)

# Optionally, convert the result back to a pandas DataFrame
# You might want to create column names based on TF-IDF features and the target
feature_names = tfidf_vectorizer.get_feature_names_out()
new_column_names = list(feature_names) + ['target']

new_df = pd.DataFrame(new_df_array, columns=new_column_names)

print("\nShape of the new DataFrame:", new_df.shape)
print("\nFirst 5 rows of the new DataFrame:")
new_df.head()

# prompt: distribution of X with respect to y

import matplotlib.pyplot as plt
sns.countplot(x='target', data=df)
plt.title('Distribution of SMS Messages (0: Ham, 1: Spam)')
plt.xlabel('Message Type')
plt.ylabel('Count')
plt.show()



# prompt: smote

!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Split the data into features (X) and target (y)
X = tfidf_matrix
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Shape of X_train before SMOTE:", X_train.shape)
print("Shape of X_train after SMOTE:", X_train_resampled.shape)
print("Shape of y_train before SMOTE:", y_train.shape)
print("Shape of y_train after SMOTE:", y_train_resampled.shape)

# Check the class distribution after SMOTE
print("\nClass distribution of y_train before SMOTE:")
print(y_train.value_counts())

print("\nClass distribution of y_train after SMOTE:")
print(y_train_resampled.value_counts())



# prompt: print shape of train and test data

print("Shape of training data (X_train):", X_train.shape)
print("Shape of testing data (X_test):", X_test.shape)
print("Shape of training labels (y_train):", y_train.shape)
print("Shape of testing labels (y_test):", y_test.shape)

# prompt: build logistic regression model

from sklearn.linear_model import LogisticRegression

# Build the Logistic Regression model
model = LogisticRegression(random_state=42)

# Train the model on the resampled training data
model.fit(X_train_resampled, y_train_resampled)

# prompt: predict with x_test

y_pred = model.predict(X_test)
y_pred

# prompt: confusion matrix and accuracy

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)
print("\nAccuracy:", accuracy)

# Print the classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# prompt: predict
# prompt: pipeline structure for nlp preprocessing steps, if we give new input  sms, it has to pass through all the given process to be applied for model training and prediction

def preprocess_sms(sms_text):
    """
    Applies the defined preprocessing steps to a single SMS message.

    Args:
        sms_text (str): The input SMS message string.

    Returns:
        str: The processed text string, ready for vectorization.
    """
    # 1. Remove special characters and numbers
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', sms_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # 2. Tokenization
    tokens = nltk.word_tokenize(cleaned_text)

    # 3. Removing Stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # 4. Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]

    # Convert tokens back to a string for vectorization
    processed_text = ' '.join(tokens)

    return processed_text

# Example usage with a new SMS:
new_sms = "Congratulations! You've won a free iPhone! Call now to claim."
processed_new_sms = preprocess_sms(new_sms)
print(f"Original SMS: {new_sms}")
print(f"Processed SMS: {processed_new_sms}")

# To apply this to a new SMS for model prediction:
# First, preprocess the new SMS
processed_sms_for_prediction = preprocess_sms(new_sms)

# Then, vectorize the processed SMS using the fitted TF-IDF vectorizer
# Note: Use transform, not fit_transform, as the vocabulary is already learned
vectorized_sms = tfidf_vectorizer.transform([processed_sms_for_prediction])

# The 'vectorized_sms' is now ready to be fed into your trained Logistic Regression model
# Assuming you have a trained model variable named 'model':
# prediction = model.predict(vectorized_sms)
# prediction_proba = model.predict_proba(vectorized_sms)

# print(f"Vectorized SMS shape: {vectorized_sms.shape}")
# print(f"Prediction: {prediction}")
# print(f"Prediction Probability: {prediction_proba}")


# Function to predict the label of a new SMS






def predict_sms(sms_text, model, vectorizer):
    """
    Predicts whether a new SMS message is spam or ham.

    Args:
        sms_text (str): The input SMS message string.
        model: The trained Logistic Regression model.
        vectorizer: The fitted TF-IDF vectorizer.

    Returns:
        str: 'spam' or 'ham'.
    """
    # Preprocess the new SMS
    processed_sms = preprocess_sms(sms_text)
    # Vectorize the processed SMS
    vectorized_sms = vectorizer.transform([processed_sms])

    # Predict the label
    prediction = model.predict(vectorized_sms)

    # Decode the predicted label back to 'ham' or 'spam'
    predicted_label = le.inverse_transform(prediction)[0]

    return predicted_label

# Example usage with a new SMS:
new_sms_to_predict = "Free entry in a contest to win FABULOUS prizes! text START to 87121 now! Cost 10p/msg. T&C's apply."
predicted_label = predict_sms(new_sms_to_predict, model, tfidf_vectorizer)

print(f"\nNew SMS: {new_sms_to_predict}")
print(f"Predicted Label: {predicted_label}")

# Example with a ham message
new_sms_to_predict_ham = "Hey, just checking in. How are you doing today?"
predicted_label_ham = predict_sms(new_sms_to_predict_ham, model, tfidf_vectorizer)

print(f"\nNew SMS: {new_sms_to_predict_ham}")
print(f"Predicted Label: {predicted_label_ham}")

def preprocess_sms(sms_text):
    """
    Applies the defined preprocessing steps to a single SMS message.

    Args:
        sms_text (str): The input SMS message string.

    Returns:
        str: The processed text string, ready for vectorization.
    """
    # 1. Remove special characters and numbers
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', sms_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # 2. Tokenization
    tokens = nltk.word_tokenize(cleaned_text)

    # 3. Removing Stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # 4. Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]

    # Convert tokens back to a string for vectorization
    processed_text = ' '.join(tokens)

    return processed_text

def predict_sms(sms_text, model, vectorizer):
    """
    Predicts whether a new SMS message is spam or ham.

    Args:
        sms_text (str): The input SMS message string.
        model: The trained Logistic Regression model.
        vectorizer: The fitted TF-IDF vectorizer.

    Returns:
        str: 'spam' or 'ham'.
    """
    # Preprocess the new SMS
    processed_sms = preprocess_sms(sms_text)
    # Vectorize the processed SMS
    vectorized_sms = vectorizer.transform([processed_sms])

    # Predict the label
    prediction = model.predict(vectorized_sms)

    # Decode the predicted label back to 'ham' or 'spam'
    predicted_label = le.inverse_transform(prediction)[0]

    return predicted_label

# prompt: tune logistic regression model

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Logistic Regression
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength
    'penalty': ['l1', 'l2'],     # Regularization penalty
    'solver': ['liblinear']      # Solver that supports both l1 and l2
}

# Create the GridSearchCV object
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the grid search to the resampled training data
grid_search.fit(X_train_resampled, y_train_resampled)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best parameters found by GridSearchCV:")
print(best_params)
print("Best accuracy score found on training data (with CV):")
print(best_score)

# Get the best model from the grid search
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_tuned = best_model.predict(X_test)

print("\nEvaluation of the tuned Logistic Regression model on the test set:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_tuned))
print("\nAccuracy:", accuracy_score(y_test, y_pred_tuned))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_tuned))

# You can now use 'best_model' for predictions
# Example usage with a new SMS using the tuned model:
new_sms_to_predict_tuned = "Free entry in a contest to win FABULOUS prizes! text START to 87121 now! Cost 10p/msg. T&C's apply."
predicted_label_tuned = predict_sms(new_sms_to_predict_tuned, best_model, tfidf_vectorizer)

print(f"\nNew SMS (using tuned model): {new_sms_to_predict_tuned}")
print(f"Predicted Label (using tuned model): {predicted_label_tuned}")

# Example with a ham message using the tuned model
new_sms_to_predict_ham_tuned = "Hey, just checking in. How are you doing today?"
predicted_label_ham_tuned = predict_sms(new_sms_to_predict_ham_tuned, best_model, tfidf_vectorizer)

print(f"\nNew SMS (using tuned model): {new_sms_to_predict_ham_tuned}")
print(f"Predicted Label (using tuned model): {predicted_label_ham_tuned}")



# prompt: save this model

import joblib

# Save the trained model
joblib.dump(best_model, 'logistic_regression_sms_spam_model.pkl')
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')
joblib.dump(le, 'label_encoder.pkl')

print("Model, TF-IDF vectorizer, and Label Encoder saved successfully.")

# prompt: save this model

import joblib

# Save the trained model
joblib.dump(best_model, 'logistic_regression_sms_spam_model.pkl')
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')
joblib.dump(le, 'label_encoder.pkl')

print("Model, TF-IDF vectorizer, and Label Encoder saved successfully.")

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# 
# # Download necessary NLTK data
# nltk.download('punkt', quiet=True)
# nltk.download('stopwords', quiet=True)
# nltk.download('wordnet', quiet=True)
# 
# # Load the trained model, vectorizer, and label encoder
# try:
#     model = joblib.load('logistic_regression_sms_spam_model.pkl')
#     tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')
#     le = joblib.load('label_encoder.pkl')
# except FileNotFoundError:
#     st.error("Model, vectorizer, or label encoder files not found. Please ensure they are saved in the correct location.")
#     st.stop()
# 
# lemmatizer = WordNetLemmatizer()
# stop_words = set(stopwords.words('english'))
# 
# def preprocess_sms(sms_text):
#     """
#     Applies the defined preprocessing steps to a single SMS message.
# 
#     Args:
#         sms_text (str): The input SMS message string.
# 
#     Returns:
#         str: The processed text string, ready for vectorization.
#     """
#     # 1. Remove special characters and numbers
#     cleaned_text = re.sub(r'[^a-zA-Z\s]', '', sms_text)
#     cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
# 
#     # 2. Tokenization
#     tokens = nltk.word_tokenize(cleaned_text)
# 
#     # 3. Removing Stopwords
#     tokens = [word for word in tokens if word.lower() not in stop_words]
# 
#     # 4. Lemmatization
#     tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]
# 
#     # Convert tokens back to a string for vectorization
#     processed_text = ' '.join(tokens)
# 
#     return processed_text
# 
# def predict_sms(sms_text, model, vectorizer, label_encoder):
#     """
#     Predicts whether a new SMS message is spam or ham.
# 
#     Args:
#         sms_text (str): The input SMS message string.
#         model: The trained Logistic Regression model.
#         vectorizer: The fitted TF-IDF vectorizer.
#         label_encoder: The fitted Label Encoder.
# 
# 
#     Returns:
#         str: 'spam' or 'ham'.
#     """
#     # Preprocess the new SMS
#     processed_sms = preprocess_sms(sms_text)
#     if not processed_sms: # Handle empty string after preprocessing
#         return "Cannot classify empty or highly filtered message."
# 
#     # Vectorize the processed SMS
#     vectorized_sms = vectorizer.transform([processed_sms])
# 
#     # Predict the label
#     prediction = model.predict(vectorized_sms)
# 
#     # Decode the predicted label back to 'ham' or 'spam'
#     predicted_label = label_encoder.inverse_transform(prediction)[0]
# 
#     return predicted_label
# 
# # Streamlit App
# st.title("SMS Spam Detection")
# 
# st.write("Enter an SMS message below to check if it is Spam or Ham.")
# 
# sms_input = st.text_area("Enter SMS Message:", height=150)
# 
# if st.button("Predict"):
#     if sms_input:
#         prediction = predict_sms(sms_input, model, tfidf_vectorizer, le)
#         if prediction == 'spam':
#             st.error(f"Prediction: {prediction.upper()}")
#         else:
#             st.success(f"Prediction: {prediction.upper()}")
#     else:
#         st.warning("Please enter an SMS message to predict.")
# 
#

# @title
!pip install streamlit -q
!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
!dpkg -i cloudflared-linux-amd64.deb

!pkill streamlit || echo "No previous Streamlit process"
import time, subprocess

streamlit_proc = subprocess.Popen(["streamlit", "run", "app.py", "--server.port", "8501", "--server.headless", "true"])
time.sleep(8)

# Create tunnel
!cloudflared tunnel --url http://localhost:8501 --no-autoupdate

"""#END"""

# prompt: get ip address for tunnel password curl ipv6

!curl ifconfig.me

!streamlit run app.py & npx localtunnel --port 8501



!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sam.py
# import streamlit as st
# 
# st.title("Streamlit in Colab")
# name = st.text_input("Enter your name")
# if name:
#     st.write(f"Hello, {name} ðŸ‘‹")
#

from pyngrok import ngrok
import subprocess

# Kill previous tunnels
ngrok.kill()

# Start Streamlit app


# Get public URL

url = ngrok.connect(8501)
print("Streamlit App:", url)

!streamlit run sam.py &

